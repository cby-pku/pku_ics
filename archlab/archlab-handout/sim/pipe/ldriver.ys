#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
# Boyuan Chen 2200017816
# Describe how and why you modified the baseline code.
# 1. add iaddq instruction in pipe-full.hcl
# 2. using load forwarding 
# 3. using 8-road expansion+rem specal management



##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header
	iaddq $-8,%rdx		# judgement of len
	jl Re
			# if so, goto Done:

Loop:	
	mrmovq (%rdi),%r8
	rmmovq %r8,(%rsi)
	andq %r8,%r8
	jle N8
	iaddq $1,%rax

N8:
	mrmovq 8(%rdi),%r8
	rmmovq %r8,8(%rsi)
	andq %r8,%r8
	jle N9
	iaddq $1,%rax
N9:
	mrmovq 16(%rdi),%r8
	rmmovq %r8,16(%rsi)
	andq %r8,%r8
	jle N10
	iaddq $1,%rax
N10:
	mrmovq 24(%rdi),%r8
	rmmovq %r8,24(%rsi)
	andq %r8,%r8
	jle N11
	iaddq $1,%rax
N11:
	mrmovq 32(%rdi),%r8
	rmmovq %r8,32(%rsi)
	andq %r8,%r8
	jle N12
	iaddq $1,%rax
N12:
	mrmovq 40(%rdi),%r8
	rmmovq %r8,40(%rsi)
	andq %r8,%r8
	jle N13
	iaddq $1,%rax
N13:
	mrmovq 48(%rdi),%r8
	rmmovq %r8,48(%rsi)
	andq %r8,%r8
	jle N14
	iaddq $1,%rax
N14:
	mrmovq 56(%rdi),%r8
	rmmovq %r8,56(%rsi)
	andq %r8,%r8
	jle N15
	iaddq $1,%rax
N15:
	iaddq $64,%rdi
	iaddq $64,%rsi
	iaddq $-8,%rdx
	jge Loop

	

#the fix of r
Re:
	iaddq $4,%rdx
	jl Re1
	je R4
	iaddq $-2,%rdx
	jl R5
	je R6
	jmp R7

Re1:
	iaddq $3,%rdx
	je R1
	iaddq $-1,%rdx
	je R2
	jg R3
	jmp Done


R7:
	mrmovq 48(%rdi),%r8
	rmmovq %r8,48(%rsi)
	andq %r8,%r8
	jle R6
	iaddq $1,%rax
R6:
	mrmovq 40(%rdi),%r8
	rmmovq %r8,40(%rsi)
	andq %r8,%r8
	jle R5
	iaddq $1,%rax
R5:
	mrmovq 32(%rdi),%r8
	rmmovq %r8,32(%rsi)
	andq %r8,%r8
	jle R4
	iaddq $1,%rax
R4:
	mrmovq 24(%rdi),%r8
	rmmovq %r8,24(%rsi)
	andq %r8,%r8
	jle R3
	iaddq $1,%rax
R3:
	mrmovq 16(%rdi),%r8
	rmmovq %r8,16(%rsi)
	andq %r8,%r8
	jle R2
	iaddq $1,%rax
R2:
	mrmovq 8(%rdi),%r8
	rmmovq %r8,8(%rsi)
	andq %r8,%r8
	jle R1
	iaddq $1,%rax
R1:
	mrmovq (%rdi),%r8
	rmmovq %r8,(%rsi)
	andq %r8,%r8
	jle Done
	iaddq $1,%rax
##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad -2
	.quad 3
	.quad 4
	.quad 5
	.quad -6
	.quad 7
	.quad 8
	.quad -9
	.quad 10
	.quad -11
	.quad 12
	.quad -13
	.quad -14
	.quad -15
	.quad -16
	.quad -17
	.quad -18
	.quad -19
	.quad -20
	.quad 21
	.quad 22
	.quad -23
	.quad -24
	.quad -25
	.quad -26
	.quad -27
	.quad -28
	.quad -29
	.quad -30
	.quad 31
	.quad -32
	.quad 33
	.quad 34
	.quad -35
	.quad 36
	.quad -37
	.quad -38
	.quad 39
	.quad 40
	.quad 41
	.quad -42
	.quad -43
	.quad 44
	.quad -45
	.quad -46
	.quad 47
	.quad 48
	.quad -49
	.quad 50
	.quad -51
	.quad -52
	.quad 53
	.quad -54
	.quad 55
	.quad 56
	.quad 57
	.quad 58
	.quad 59
	.quad 60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
